# -*- coding: utf-8 -*-
"""run.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jt2IWTxtNjZlTsEwiVdjokvHkTWs7tu2
"""

#! /opt/bin/nvidia-smi

'''from google.colab import drive
import os
drive.mount('/content/drive')
os.chdir('/content')

import shutil
DIR = '/content/drive/MyDrive/Colab Notebooks/HW3'
shutil.copyfile(os.path.join(DIR, 'HW3.zip'), 'HW3.zip')
!unzip HW3.zip

os.chdir('/content/HW3')'''

from PixelLib.pixellib.custom_train import instance_custom_training
from utils import get_imagenet_weights
import glob

class_names = ['BG', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']
save_models_path = 'mask_rcnn_models512'

def get_bestModel_weights():
  return os.path.join(os.path.abspath(save_models_path), sorted(os.listdir(save_models_path), reverse=True)[0])
   
#params
init_lr = 0.005
lr_steps = 20
max_dim = 512
min_dim = 512

#infer
def infer():
  train_maskrcnn = instance_custom_training()
  train_maskrcnn.load_dataset("dataset")
  train_maskrcnn.modelConfig(network_backbone="resnet101", num_classes=20, class_names=class_names, batch_size=1, detection_threshold=0.5, image_max_dim=max_dim, image_min_dim=min_dim)
  #train_maskrcnn.evaluate_model(model_path=save_models_path)
  train_maskrcnn.evaluate_model(model_path=get_bestModel_weights())

#train
train_maskrcnn = instance_custom_training()
train_maskrcnn.load_dataset("dataset")


#1st
train_maskrcnn.modelConfig(network_backbone="resnet101", num_classes=20, class_names=class_names, batch_size=16, detection_threshold=0.7, image_max_dim=max_dim, image_min_dim=min_dim, learning_rate=init_lr, lr_steps=lr_steps)
train_maskrcnn.load_pretrained_model(get_imagenet_weights())
train_maskrcnn.train_model(num_epochs=30, augmentation=True, layers='heads', path_trained_models=save_models_path)
infer()
#2nd
train_maskrcnn.modelConfig(network_backbone="resnet101", num_classes=20, class_names=class_names, batch_size=16, detection_threshold=0.7, image_max_dim=max_dim, image_min_dim=min_dim, learning_rate=init_lr, lr_steps=lr_steps)
train_maskrcnn.load_pretrained_model(get_bestModel_weights())
train_maskrcnn.train_model(num_epochs=60, augmentation=True, layers='4+', path_trained_models=save_models_path)
infer()

#3st
train_maskrcnn.modelConfig(network_backbone="resnet101", num_classes=20, class_names=class_names, batch_size=8, detection_threshold=0.7, image_max_dim=max_dim, image_min_dim=min_dim, learning_rate=init_lr/10, lr_steps=lr_steps)
train_maskrcnn.load_pretrained_model(get_bestModel_weights())
train_maskrcnn.train_model(num_epochs=200, augmentation=True, layers='all', path_trained_models=save_models_path)
infer()
'''
#3st
train_maskrcnn.modelConfig(network_backbone="resnet101", num_classes=20, class_names=class_names, batch_size=8, detection_threshold=0.5, image_max_dim=max_dim, image_min_dim=min_dim, learning_rate=init_lr, lr_steps=lr_steps)
train_maskrcnn.load_pretrained_model(get_imagenet_weights())
train_maskrcnn.train_model(num_epochs=30, augmentation=True, layers='all', path_trained_models=save_models_path)
'''


import numpy as np
from itertools import groupby
from pycocotools import mask as maskutil

def binary_mask_to_rle(binary_mask):
    rle = {'counts': [], 'size': list(binary_mask.shape)}
    counts = rle.get('counts')
    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):
        if i == 0 and value == 1:
            counts.append(0)
        counts.append(len(list(elements)))
    compressed_rle = maskutil.frPyObjects(rle, rle.get('size')[0], rle.get('size')[1])
    compressed_rle['counts'] = str(compressed_rle['counts'], encoding='utf-8')
    return compressed_rle

from PixelLib.pixellib.instance import custom_segmentation 
from utils import binary_mask_to_rle
import json, os
import numpy as np

json_path = 'dataset/test.json'
with open(json_path) as f:
   test_json = json.load(f)

test_path = 'dataset/test'
save_dir = 'test_predict'
if not os.path.exists(save_dir):
  os.mkdir(save_dir)

submissions = []

segment_image = custom_segmentation()
segment_image.inferConfig(num_classes=20, class_names=class_names, image_max_dim=max_dim, image_min_dim=min_dim, detection_threshold=0.5)
segment_image.load_model(get_bestModel_weights())

for t in test_json['images']:
  filename = t['file_name']
  id = t['id']
  filepath = os.path.join(test_path, filename)
  save_path = os.path.join(save_dir, filename)
  results, o = segment_image.segmentImage(filepath, show_bboxes=True, output_image_name=save_path)
  #print(results)
  category_ids = results['class_ids']
  scores = results['scores']
  segmentations = results['masks']
  n_instances = len(scores)    
  if len(category_ids) > 0: # If any objects are detected in this image
    for i in range(n_instances):
      instance = dict()
      category_id, score, segmentation = category_ids[i], scores[i], binary_mask_to_rle(segmentations[:,:,i])
      #print(segmentations[:,:,i])
      instance['image_id'], instance['category_id'], instance['segmentation'], instance['score'] = \
        int(id), int(category_id), segmentation, float(score)
      submissions.append(instance)
      #print(segmentation)

#submissions = tuple(submissions)
with open(os.path.join(DIR,"submission.json"), "w") as f:
    json.dump(submissions, f)
